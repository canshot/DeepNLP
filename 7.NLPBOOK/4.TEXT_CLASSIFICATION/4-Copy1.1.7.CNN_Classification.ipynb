{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카글 텍스트 분류 - 합성곱 신경망 활용 접근방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import string\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH ='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "FILE_DIR_PATH = './data_in/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'\n",
    "\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
    "\n",
    "train_input_data = np.load(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "train_label_data = np.load(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))\n",
    "\n",
    "prepro_configs = None\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라메터 변수\n",
    "RNG_SEED = 1234\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "VOCAB_SIZE = len(prepro_configs)\n",
    "EMB_SIZE = 128\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_SEQ_LEN = 604 # 문장 최대 길이\n",
    "\n",
    "input_train, input_valid, label_train, label_valid = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "#문장 길이 구하는 값, 전처리로 같은 길이를 맞추어 놔서 의미 없음\n",
    "len_train = np.array([min(len(x), MAX_SEQ_LEN) for x in input_train])\n",
    "len_valid = np.array([min(len(x), MAX_SEQ_LEN) for x in input_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_train))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_eval))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer를 선언합니다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    VOCAB_SIZE,\n",
    "                    EMB_SIZE,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "    \n",
    "    embed_seq = layers.Reshape((MAX_SEQ_LEN, EMB_SIZE, 1))(input_layer)\n",
    "    \n",
    "    # 현재 모델이 학습모드인지 여부를 확인하는 변수입니다.\n",
    "        \n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.5,\n",
    "                                   training=training)\n",
    "    \n",
    "    \n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1, name='logits')\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
    "    \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    elif PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 100, '_session_config': None, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10bec7128>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "config_tf = tf.estimator.RunConfig()\n",
    "config_tf._save_checkpoints_secs = 100\n",
    "config_tf._keep_checkpoint_max =  2\n",
    "config_tf._log_step_count_steps = 100\n",
    "\n",
    "cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config_tf, params=params)\n",
    "\n",
    "# class EarlyStoppingLossHook(tf.train.SessionRunHook):\n",
    "#     def __init__(self, loss_tensor_name, value, threshold=3):\n",
    "#         '''\n",
    "#         A train hook to stop the training at specified train loss\n",
    "#         Usage:\n",
    "#         loss_monitor = EarlyStoppingLossHook(\"reduced_mean:0\", 0.35, 3)\n",
    "#         estimator.train(input_fn=train_input_fn, hooks=[loss_monitor], ...)\n",
    "\n",
    "#         :param loss_tensor_name: Name of the loss tensor eg: loss:0\n",
    "#         :param value: Value at which the trianing should stop\n",
    "#         :param threshold: number of times to check for the loss value, before stopping the training\n",
    "#         '''\n",
    "#         self._best_loss = value\n",
    "#         self.threshold = threshold\n",
    "#         self.count  = 0\n",
    "#         self.loss_tensor_name = loss_tensor_name\n",
    "#         logging.info(\"Create EarlyStoppingLossHook for {}\".format(self.loss_tensor_name))\n",
    "\n",
    "#     def before_run(self, run_context):\n",
    "#         graph = run_context.session.graph\n",
    "#         tensor_name = self.loss_tensor_name\n",
    "#         loss_tensor_name = graph.get_tensor_by_name(tensor_name)\n",
    "#         return session_run_hook.SessionRunArgs(loss_tensor_name)\n",
    "\n",
    "#     def after_run(self, run_context, run_values):\n",
    "#         last_loss = run_values.results\n",
    "\n",
    "#         if last_loss <= self._best_loss:\n",
    "#             self.count += 1\n",
    "#             if self.count == self.threshold:\n",
    "#                 logging.info(\"EarlyStoppingHook: Request early stop\")\n",
    "#                 run_context sss.request_stop()\n",
    "\n",
    "# early_stopping = EarlyStoppingLossHook('sigmoid_cross_entropy_loss/value:0', 0.1, 5)\n",
    "\n",
    "# train_spec = tf.estimator.TrainSpec(input_fn=custom_input_fn(X=input_train, y=label_train, is_training=True), max_steps=NUM_EPOCHS, hooks=[early_stopping])\n",
    "# eval_spec = tf.estimator.EvalSpec(input_fn=custom_input_fn(X=input_train, y=label_train, is_training=True))\n",
    "\n",
    "# tf.estimator.train_and_evaluate(cnn_est, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-278\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 278 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.87576187, step = 279\n",
      "INFO:tensorflow:global_step/sec: 1.2192\n",
      "INFO:tensorflow:loss = 0.8132618, step = 379 (82.023 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 394 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.14472\n",
      "INFO:tensorflow:loss = 0.78982425, step = 479 (87.357 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 512 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.21579\n",
      "INFO:tensorflow:loss = 0.75076175, step = 579 (82.253 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 642 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.30748\n",
      "INFO:tensorflow:loss = 0.83669937, step = 679 (76.480 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 774 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.31823\n",
      "INFO:tensorflow:loss = 0.77419925, step = 779 (75.859 sec)\n"
     ]
    }
   ],
   "source": [
    "cnn_est.train(train_input_fn)\n",
    "cnn_est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object Estimator.predict at 0x112929db0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 571, in predict\n",
      "    for key, value in six.iteritems(preds_evaluated)\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5023, in get_controller\n",
      "    yield g\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4839, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n",
      "Exception ignored in: <generator object Estimator.predict at 0x12af53af0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 571, in predict\n",
      "    for key, value in six.iteritems(preds_evaluated)\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5023, in get_controller\n",
      "    yield g\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4839, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n",
      "Exception ignored in: <generator object Estimator.predict at 0x12af53830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 571, in predict\n",
      "    for key, value in six.iteritems(preds_evaluated)\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5023, in get_controller\n",
      "    yield g\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4839, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n",
      "Exception ignored in: <generator object Estimator.predict at 0x12af5f7d8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 571, in predict\n",
      "    for key, value in six.iteritems(preds_evaluated)\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 5023, in get_controller\n",
      "    yield g\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/Users/sinseongjin/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4839, in get_controller\n",
      "    type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "prediction_fn = cnn_est.predict(input_fn=input_fn(X=test_input_data, y=None, is_training=False))\n",
    "predictions = np.array([p['prob'][0] for p in cnn_est.predict(input_fn=input_fn(X=test_input_data, y=None, is_training=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 'IteratorGetNext:0'}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-278\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    cnn_est.train(custom_input_fn(X=input_train, y=label_train, is_training=True))\n",
    "    cnn_est.evaluate(custom_input_fn(X=input_valid, y=label_valid, is_training=True))\n",
    "    \n",
    "    # name scopes의 재사용을 위해 graph를 reset한다.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool),\n",
    "                             num_thresholds=21)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#테스트 데이터 로드\n",
    "test = pd.read_csv(DEFAULT_PATH+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print (\"test dataset shape: {}\".format(test.shape))\n",
    "\n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":list(predictions)} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv(\"./data_out/Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find trained model in model_dir: /Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/cnn, running initialization to predict.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "test dataset shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb')) #테스트데이터 로드\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False) #numpy 형태로 저장\n",
    "cnn_classifier.predict(input_fn=predict_input_fn)\n",
    "\n",
    "predictions = np.array([p['logits'][0] for p in cnn_classifier.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               truncating='post',\n",
    "                               padding='post',\n",
    "                               value=pad_id)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "#             predictions[path][idx]\n",
    "    \n",
    "    return predictions[path][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-dd869d8851b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Current Progress %d \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msentimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_test_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36mprint_predictions\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     x = sequence.pad_sequences(indexes, \n\u001b[1;32m     12\u001b[0m                                \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     x = sequence.pad_sequences(indexes, \n\u001b[1;32m     12\u001b[0m                                \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36mtext_to_index\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moov_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-aa33724cb1ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moov_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "\n",
    "sentimental = []\n",
    "\n",
    "for i in range(len(clean_test_reviews)):\n",
    "    if ( (i+1) % 1000 == 0):\n",
    "        print (\"Current Progress %d \\n\" % (i+1))\n",
    "    sentimental.append(print_predictions([clean_test_reviews[i]]))\n",
    "    \n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":sentimental} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv( \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function custom_input_fn.<locals>.internal_input_fn at 0x10be8ce18>, X=array([[4353,  728,    1, ...,    0,    0,    0],\n",
       "       [4874, 6507,   62, ...,    0,    0,    0],\n",
       "       [8817,  290, 3665, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  16, 6306,    2, ...,    0,    0,    0],\n",
       "       [1249, 1972,    1, ...,    0,    0,    0],\n",
       "       [1243,  347,  462, ...,    0,    0,    0]], dtype=int32), y=array([1, 1, 0, ..., 1, 1, 0]), is_training=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_input_fn(X, y=None, is_training=False):\n",
    "\n",
    "    def internal_input_fn(X, y=None, is_training=False):\n",
    "        \n",
    "        if (not isinstance(X, dict)):\n",
    "            X = {\"x\": X}\n",
    "        \n",
    "        if (y is None):\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "        \n",
    "        if (is_training):\n",
    "            dataset = dataset.repeat().shuffle(len(X['x']))\n",
    "            batch_size = BATCH_SIZE\n",
    "        else:\n",
    "            batch_size = 1\n",
    "\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset_iter = dataset.make_initializable_iterator()\n",
    "\n",
    "        if (y is None):\n",
    "            features = dataset_iter.get_next()\n",
    "            labels = None\n",
    "        else:\n",
    "            features, labels = dataset_iter.get_next()\n",
    "\n",
    "        input_tensor_map = dict()\n",
    "        for input_name, tensor in features.items():\n",
    "            input_tensor_map[input_name] = tensor.name\n",
    "            \n",
    "        print(input_tensor_map)\n",
    "\n",
    "        with open(os.path.join(FILE_DIR_PATH, 'input_tensor_map.pickle'), 'wb') as f:\n",
    "            pickle.dump(input_tensor_map, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, dataset_iter.initializer)\n",
    "        \n",
    "        return (features, labels) if (not labels is None) else features\n",
    "    \n",
    "    return partial(internal_input_fn, X=X, y=y, is_training=is_training)\n",
    "\n",
    "custom_input_fn(input_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classification\n",
    "\n",
    "CNN을 활용하여 text를 분류해보자, n-gram의 효과로 활용\n",
    "\n",
    "https://www.semanticscholar.org/paper/Learning-to-Rank-Short-Text-Pairs-with-Deep-Neural-Severyn-Moschitti/452f7411af7d471dd3ba84c2b06b2aaffc38cdb9\n",
    "\n",
    "Embedding Layer -> Dropout -> Conv1D -> GlobalMax1D -> Hidden Dense Layer -> Dropout -> Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = {}\n",
    "\n",
    "def train_and_evaluate(classifier):\n",
    "    # 예측 테스트를 위해 모델을 학습시키고 저장한다.\n",
    "    all_classifiers[classifier.model_dir] = classifier\n",
    "    classifier.train(input_fn=train_input_fn, steps=1)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    predictions = np.array([p['logistic'][0] for p in classifier.predict(input_fn=eval_input_fn)])\n",
    "    \n",
    "    # name scopes의 재사용을 위해 graph를 reset한다.\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    pr = summary_lib.pr_curve('precision_recall', predictions=predictions, labels=y_test.astype(bool),\n",
    "                             num_thresholds=21)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter(os.path.join(classifier.model_dir, 'eval'), sess.graph)\n",
    "        writer.add_summary(sess.run(pr), global_step=0)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/user/git/DeepNLP/7.NLPBOOK/5.TEXT_CLASSIFICATION/checkpoint/cnn_model/cnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11776c978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#head: pre-made estimator로 평가를 할 때, 일정한 함수를 사용하게 세팅\n",
    "head = tf.contrib.estimator.binary_classification_head()\n",
    "\n",
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    #embedding layer를 선언한다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    vocab_size,\n",
    "                    EMB_SIZE,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.2,\n",
    "                                   training=training)\n",
    "\n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)  \n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1)\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer() #여러가지 Optimizer 활용가능\n",
    "    \n",
    "    def _train_op_fn(loss):\n",
    "#         tf.summary('loss', loss)\n",
    "        return optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "\n",
    "    \n",
    "    return head.create_estimator_spec(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        mode=mode,\n",
    "        logits=logits,\n",
    "        train_op_fn=_train_op_fn)\n",
    "\n",
    "\n",
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        model_dir=os.path.join(model_dir, 'cnn'),\n",
    "                                        params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_and_evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b31a89aa73fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#학습 후, 결과치를 tensorboard로 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tensorboard --logdir=./checkpoint/cnn_classifier/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_and_evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "#학습 후, 결과치를 tensorboard로 확인\n",
    "# tensorboard --logdir=./checkpoint/cnn_classifier/\n",
    "train_and_evaluate(cnn_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-72e06e092849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logistic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnn_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = np.array([p['logistic'][0] for p in cnn_classifier.predict(input_fn=eval_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 prediction으로 테스트 해 본다\n",
    "\n",
    "def text_to_index(sentence):\n",
    "    # Remove punctuation characters except for the apostrophe\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\"'\", ''))\n",
    "    tokens = sentence.translate(translator).lower().split()\n",
    "    return np.array([1] + [word_index[t] if t in word_index else oov_id for t in tokens])\n",
    "\n",
    "def print_predictions(sentences):\n",
    "    indexes = [text_to_index(sentence) for sentence in sentences]\n",
    "    x = sequence.pad_sequences(indexes, \n",
    "                               maxlen=sentence_size, \n",
    "                               truncating='post',\n",
    "                               padding='post',\n",
    "                               value=pad_id)\n",
    "    length = np.array([min(len(x), sentence_size) for x in indexes])\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": x, \"len\": length}, shuffle=False)\n",
    "    predictions = {}\n",
    "    for path, classifier in all_classifiers.items():\n",
    "        predictions[path] = [p['logistic'][0] for p in classifier.predict(input_fn=predict_input_fn)]\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        print(sentence)\n",
    "        for path in all_classifiers:\n",
    "            print(\"\\t{} {}\".format(path, predictions[path][idx]))\n",
    "#             predictions[path][idx]\n",
    "    \n",
    "    return predictions[path][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions([\n",
    "    'I do not like this movie'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions(['fuck you', 'this movie sucks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 했던 것을 모두 활용하여 제출용 데이터를 만들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "default_path = '/Users/user/.kaggle/competitions/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 데이터 로드\n",
    "test = pd.read_csv(default_path+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print (\"test dataset shape: {}\".format(test.shape))\n",
    "\n",
    "# 불용어 제거 및 태그를 삭제 후, 데이터를 저장할 장소를 만들자\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = []\n",
    "\n",
    "print (\"테스트 영화 리뷰 전처리 진행...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print (\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_to_words( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 파일은 이렇게 생겼다고 합니다.\n",
    "print (test.head())\n",
    "\n",
    "#이 파일은 \"sentiment\" 행이 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "\n",
    "sentimental = []\n",
    "\n",
    "for i in range(len(clean_test_reviews)):\n",
    "    if ( (i+1) % 1000 == 0):\n",
    "        print (\"Current Progress %d \\n\" % (i+1))\n",
    "    sentimental.append(print_predictions([clean_test_reviews[i]]))\n",
    "    \n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":sentimental} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv( \"Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-24-45b20891aaa6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-45b20891aaa6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    output.to_csv( \"final_bof.csv\", index=False, quoting=3 )'\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":final_result} )\n",
    "\n",
    "#결과값 저장\n",
    "output.to_csv( \"final_bof.csv\", index=False, quoting=3 )'\n",
    "\n",
    "#0.5 기준으로 값들을 변환\n",
    "\n",
    "def correct_val(x):\n",
    "    if x >= 0.5:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    \n",
    "    return x\n",
    "\n",
    "final_result = output['sentiment'].apply(correct_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer를 선언합니다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    VOCAB_SIZE,\n",
    "                    EMB_SIZE,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "    # 현재 모델이 학습모드인지 여부를 확인하는 변수입니다.\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.5,\n",
    "                                   training=training)\n",
    "\n",
    "    ## filters = 32이고 kernel_size = 3이면, 길이가 3인 32개의 다른 필터를 생성합니다. 32개의 컨볼루션들을 생성합니다.\n",
    "    ## conv1d는 (배치사이즈, 길이, 채널)로 입력값을 받는데, 배치사이즈: 문장 숫자 | 길이: 각 문장의 단어의 개수 | 채널: 임베딩 출력 차원수임\n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1, name='logits')\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
    "    \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    elif PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits)\n",
    "            }\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
