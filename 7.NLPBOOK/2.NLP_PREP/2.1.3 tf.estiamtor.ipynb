{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3 tf.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 데이터를 생성합니다.\n",
    " \n",
    " - 긍,부정에 대한 문장을 간략하게 생성하였고, 첫번째가 긍정, 두번째가 부정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = ['너 오늘 이뻐보인다', \n",
    "           '나는 오늘 기분이 더러워', \n",
    "           '끝내주는데, 좋은 일이 있나봐', \n",
    "           '좋은 일이 생겼어', \n",
    "           '아 짜증나', \n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "label = [[1], [0], [1], [1],[0], [1]]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=5, padding='post')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index) + 1 #문장의 단어 사이즈\n",
    "EMB_SIZE = 128\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    input_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x'])\n",
    "    avg_pool_layer = keras.layers.GlobalAveragePooling1D()(input_layer)\n",
    "    dense_layer_1 = keras.layers.Dense(16, activation=tf.nn.relu)(avg_pool_layer)\n",
    "    dense_layer_2 = keras.layers.Dense(16, activation=tf.nn.relu)(dense_layer_1)\n",
    "    output_layer = keras.layers.Dense(2)(dense_layer_2)\n",
    "\n",
    "    one_hot_labels = tf.squeeze(tf.one_hot(labels, 2), axis=1)\n",
    "    loss = tf.losses.softmax_cross_entropy(one_hot_labels, output_layer)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./data_out/checkpoint/tutorial/\"\n",
    "\n",
    "model_basic = tf.estimator.Estimator(model_fn, model_dir=model_dir)\n",
    "model_basic.train(train_input_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
