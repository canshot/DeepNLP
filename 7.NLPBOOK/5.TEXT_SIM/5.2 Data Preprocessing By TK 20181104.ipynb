{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Data Preprocessing for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepro for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_Q1_DATA = 'train_q1.npy'\n",
    "TRAIN_Q2_DATA = 'train_q2.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "NB_WORDS_DATA = 'nb_words.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_IN_PATH + 'train.csv', encoding='utf-8')\n",
    "train_df = train_df.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neg 데이터 Data balance 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_df = train_df.loc[train_df['is_duplicate'] == 1]\n",
    "train_neg_df = train_df.loc[train_df['is_duplicate'] == 0]\n",
    "\n",
    "class_diff_count = train_neg_df.shape[0] - train_pos_df.shape[0]\n",
    "sample_frac = 1 - (class_diff_count / train_neg_df.shape[0])\n",
    "\n",
    "train_neg_df = train_neg_df.sample(frac=sample_frac)\n",
    "\n",
    "train_df = pd.concat([train_neg_df, train_pos_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_neg_df['question1']))\n",
    "print(len(train_pos_df['question1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "\n",
    "changeFilter = re.compile(FILTERS)\n",
    "\n",
    "questions = list([str(s) for s in train_df['question1']]) + list([str(s) for s in train_df['question2']])\n",
    "filtered_questions = list()\n",
    "for q in questions:\n",
    "     filtered_questions.append(re.sub(changeFilter, \"\", q).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(train_df['question1']) + list(train_df['question2'])\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(questions)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "question1_word_sequences = tokenizer.texts_to_sequences(train_df['question1'])\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(train_df['question2'])\n",
    "\n",
    "word_vocab = {}\n",
    "word_vocab = tokenizer.word_index \n",
    "print(\"Words in index: {}\".format(len(word_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_word_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_duplicate = train_df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "labels = np.array(is_duplicate, dtype=int)\n",
    "\n",
    "print('Shape of question1 data tensor: {}'.format(q1_data.shape))\n",
    "print('Shape of question2 data tensor:{}'.format(q2_data.shape))\n",
    "print('Shape of label tensor: {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) # vocab size 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_IN_PATH):\n",
    "    os.makedirs(DATA_IN_PATH)\n",
    "\n",
    "np.save(open(DATA_IN_PATH + TRAIN_Q1_DATA, 'wb'), q1_data)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_Q2_DATA, 'wb'), q2_data)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_IN_PATH + NB_WORDS_DATA, 'w') as f:\n",
    "    json.dump(data_configs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepro for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_Q1_DATA = 'test_q1.npy'\n",
    "TEST_Q2_DATA = 'test_q2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(DATA_IN_PATH + 'test.csv', encoding='utf-8')\n",
    "test_df = test_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question1_word_sequences = tokenizer.texts_to_sequences([str(s) for s in list(test_df['question1'])])\n",
    "test_question2_word_sequences = tokenizer.texts_to_sequences([str(s) for s in list(test_df['question2'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_q1_data = pad_sequences(test_question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "test_q2_data = pad_sequences(test_question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of test question1 data tensor:', test_q1_data.shape)\n",
    "print('Shape of test question2 data tensor:', test_q2_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + TEST_Q1_DATA, 'wb'), test_q1_data)\n",
    "np.save(open(DATA_IN_PATH + TEST_Q2_DATA, 'wb'), test_q2_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
