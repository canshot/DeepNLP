{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5ac1437da6fb91ecdccecb6d2382696bd31b3be"
      },
      "cell_type": "code",
      "source": "MAX_VOCAB_SIZE = 100000\nEMB_SIZE = 300\nMAX_SEQUENCE_LENGTH = 70\n\nCONV_FEATURE_DIM = 128\nCONV_WINDOW_SIZE = 5\nFC_FEATURE_DIM = 128\n\nNUM_CONV_LAYERS = 2\nNUM_FC_LAYERS = 3\n\nTHRESHOLD = 0.32",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b4dad4869df9643905cb14420234b672b910613"
      },
      "cell_type": "code",
      "source": "DATA_DIR = '../input/'\ndataset = pd.read_csv(DATA_DIR + 'train.csv')\ntest_dataset = pd.read_csv(DATA_DIR + 'test.csv')\nquestion_text = list(dataset['question_text'])\ntest_question_text = list(test_dataset['question_text'])\n\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(question_text)\nsequence = tokenizer.texts_to_sequences(question_text)\ntrain_X = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\ntrain_y = np.array(dataset['target'], dtype=np.int64)\n\ntest_sequence = tokenizer.texts_to_sequences(test_question_text)\ntest_X = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af516df8b0256613ce3a552091fe6562e7e15a2b"
      },
      "cell_type": "code",
      "source": "def load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(MAX_VOCAB_SIZE, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= MAX_VOCAB_SIZE: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(MAX_VOCAB_SIZE, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= MAX_VOCAB_SIZE: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(MAX_VOCAB_SIZE, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= MAX_VOCAB_SIZE: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a38e79a6e79e2f2ff01d052a28e4fc3441ee4a9"
      },
      "cell_type": "code",
      "source": "def f1(y_true, y_pred):\n    '''\n    metric from here \n    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n        Only computes a batch-wise average of precision.\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\nclass CyclicLR(tf.keras.callbacks.Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5729aa86fd92deedcbc5b36a2e339af43ed6506"
      },
      "cell_type": "code",
      "source": "def build_model(embedding_matrix=None):\n    \n    def conv_block(inputs):\n        conv_layer = tf.keras.layers.Conv1D(int(CONV_FEATURE_DIM / 2), \n                                            CONV_WINDOW_SIZE,  \n#                                             activation='tanh',\n                                            padding='same')(inputs)\n#         conv_layer = tf.keras.layers.Dropout(0.2)(conv_layer)\n        glu_layer = tf.keras.layers.Dense(CONV_FEATURE_DIM * 2)(conv_layer)\n        scored_output, output_layer = tf.split(glu_layer, 2, axis=-1)\n\n#         output_layer = output_layer * tf.nn.sigmoid(scored_output)\n#         output_layer = tf.keras.layers.Dense(CONV_FEATURE_DIM, activation='relu')(conv_layer)\n\n        return output_layer\n\n    def self_alignment(inputs):\n        activated_scores = tf.keras.layers.Dense(1, activation='tanh')(inputs)\n        aligned_scores = tf.nn.softmax(activated_scores, axis=1)\n        aligned_outputs = tf.reduce_sum(aligned_scores * inputs, axis=1)\n\n        return aligned_outputs\n\n    features = tf.keras.layers.Input(shape=(70,))\n    embedding_layer = tf.keras.layers.Embedding(MAX_VOCAB_SIZE, EMB_SIZE, weights=[embedding_matrix], trainable=False)(features)\n    embedding_layer = tf.keras.layers.SpatialDropout1D(0.1)(embedding_layer)\n\n    conv_embedding_layer = tf.keras.layers.Dense(CONV_FEATURE_DIM, activation='relu')(embedding_layer)\n\n    for i in range(NUM_CONV_LAYERS):\n        input_layer = conv_output_layer if i > 0 else conv_embedding_layer\n        conv_output_layer = tf.keras.layers.Lambda(lambda x: conv_block(x))(input_layer)\n        conv_output_layer = tf.keras.layers.Add()([input_layer, conv_output_layer])\n        conv_output_layer = tf.keras.layers.Dropout(0.2)(conv_output_layer)\n#         conv_output_layer = tf.keras.layers.BatchNormalization()(conv_output_layer)\n\n    attention_layer = tf.keras.layers.Lambda(lambda x: self_alignment(x))(conv_output_layer)\n    max_pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_output_layer)\n    avg_pool_layer = tf.keras.layers.GlobalAveragePooling1D()(conv_output_layer)\n\n    conv_augmented_layer = tf.keras.layers.concatenate([max_pool_layer, avg_pool_layer, attention_layer], axis=-1)\n    conv_augmented_layer = tf.keras.layers.Dense(FC_FEATURE_DIM)(conv_augmented_layer)\n\n    for i in range(NUM_FC_LAYERS):\n        input_layer = conv_fc_output_layer if i > 0 else conv_augmented_layer\n        conv_fc_output_layer = tf.keras.layers.Dense(FC_FEATURE_DIM, activation='relu')(input_layer)\n#         fc_output_layer = tf.keras.layers.Add()([input_layer, fc_output_layer])\n        conv_fc_output_layer = tf.keras.layers.Dropout(0.2)(conv_fc_output_layer)\n#         fc_output_layer = tf.keras.layers.BatchNormalization()(fc_output_layer)\n\n    rnn_output_layer = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNGRU(64, return_sequences=True))(embedding_layer)\n    rnn_attention_1_layer = tf.keras.layers.Lambda(lambda x: self_alignment(x))(rnn_output_layer)\n    rnn_output_layer = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNGRU(64, return_sequences=True))(rnn_output_layer)\n    \n    rnn_attention_2_layer = tf.keras.layers.Lambda(lambda x: self_alignment(x))(rnn_output_layer)\n    rnn_max_pool_layer = tf.keras.layers.GlobalMaxPooling1D()(rnn_output_layer)\n    rnn_avg_pool_layer = tf.keras.layers.GlobalAveragePooling1D()(rnn_output_layer)\n    \n    rnn_augmented_layer = tf.keras.layers.concatenate([rnn_max_pool_layer, rnn_avg_pool_layer, \n                                                       rnn_attention_1_layer, rnn_attention_2_layer], axis=-1)\n    rnn_augmented_layer = tf.keras.layers.Dense(FC_FEATURE_DIM)(rnn_augmented_layer)\n\n    for i in range(NUM_FC_LAYERS):\n        input_layer = rnn_fc_output_layer if i > 0 else rnn_augmented_layer\n        rnn_fc_output_layer = tf.keras.layers.Dense(FC_FEATURE_DIM, activation='relu')(input_layer)\n#         fc_output_layer = tf.keras.layers.Add()([input_layer, fc_output_layer])\n        rnn_fc_output_layer = tf.keras.layers.Dropout(0.2)(rnn_fc_output_layer)\n#         fc_output_layer = tf.keras.layers.BatchNormalization()(fc_output_layer)\n\n    augmented_layer = tf.keras.layers.concatenate([rnn_fc_output_layer, conv_fc_output_layer])\n    augmented_layer = tf.keras.layers.Dense(FC_FEATURE_DIM)(augmented_layer)\n    for i in range(NUM_FC_LAYERS):\n        input_layer = fc_output_layer if i > 0 else augmented_layer\n        fc_output_layer = tf.keras.layers.Dense(FC_FEATURE_DIM, activation='relu')(input_layer)\n#         fc_output_layer = tf.keras.layers.Add()([input_layer, fc_output_layer])\n        fc_output_layer = tf.keras.layers.Dropout(0.2)(fc_output_layer)\n#         fc_output_layer = tf.keras.layers.BatchNormalization()(fc_output_layer)\n\n    logits = tf.keras.layers.Dense(1, activation='sigmoid', name='logits')(fc_output_layer)\n    # predicts = tf.keras.layers.Lambda(lambda x: tf.round(x), name='predicts')(logits)\n    model = tf.keras.Model(inputs=features, outputs=logits)\n\n    model.compile(loss='binary_crossentropy', \n                  optimizer='adam',\n                  metrics=[f1])\n    \n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "163eb328d7589194e900734d22f4dbb8ddda1e96"
      },
      "cell_type": "code",
      "source": "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\ndef train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n    best_val_y, best_test_y, best_score = None, None, 0.\n    for e in range(epochs):\n        model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks = callback, verbose=0)\n        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n        score = metrics.f1_score(val_y, (pred_val_y > THRESHOLD).astype(int))\n        pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n        \n        if score > best_score:\n            best_score = score\n            best_val_y = pred_val_y\n            best_test_y = pred_test_y\n        \n        print(\"Epoch: \", e, \"-    Val F1 Score: {:.4f}\".format(score))\n\n    print('=' * 60)\n    return best_val_y, best_test_y, best_score\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "845c1c81bce86aae00883813481fbb0b64f1f2d3"
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nembedding_matrix_1 = load_glove(word_index)\n# embedding_matrix_3 = load_para(word_index)\n# embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_3], axis = 0)\nembedding_matrix = embedding_matrix_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "212e4166143247d8280ff475446deb7464627555"
      },
      "cell_type": "code",
      "source": "DATA_SPLIT_SEED = 2019\nclr = CyclicLR(base_lr=0.001, max_lr=0.002,\n               step_size=300., mode='exp_range',\n               gamma=0.99994)\n\ntrain_meta = np.zeros(train_y.shape)\ntest_meta = np.zeros(test_X.shape[0])\nsplits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=DATA_SPLIT_SEED).split(train_X, train_y))\nfor idx, (train_idx, valid_idx) in enumerate(splits):\n        X_train = train_X[train_idx]\n        y_train = train_y[train_idx]\n        X_val = train_X[valid_idx]\n        y_val = train_y[valid_idx]\n        model = build_model(embedding_matrix)\n        pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = 5, callback = [clr,])\n        train_meta[valid_idx] = pred_val_y.reshape(-1)\n        test_meta += pred_test_y.reshape(-1) / len(splits)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a9708b2494f3fe5df76031918101784f1ba8e0b"
      },
      "cell_type": "code",
      "source": "sub = pd.read_csv('../input/sample_submission.csv')\nsub.prediction = test_meta > THRESHOLD\nsub.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "285f48389d535557d619bb005eb237eb40870d3b"
      },
      "cell_type": "code",
      "source": "f1_score(y_true=train_y, y_pred=train_meta > THRESHOLD)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}