{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 CNN DSSM example by tf.estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "TRAIN_Q1_DATA_FILE = 'train_q1.npy'\n",
    "TRAIN_Q2_DATA_FILE = 'train_q2.npy'\n",
    "TRAIN_LABEL_DATA_FILE = 'train_label.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))\n",
    "q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))\n",
    "labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))\n",
    "prepro_configs = None\n",
    "\n",
    "with open(DATA_IN_PATH + NB_WORDS_DATA_FILE, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(prepro_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "train_Q1 = train_X[:,0]\n",
    "train_Q2 = train_X[:,1]\n",
    "test_Q1 = test_X[:,0]\n",
    "test_Q2 = test_X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.stack((q1_data, q2_data), axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.stack((q1_data, q2_data), axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rearrange(base, hypothesis, label):\n",
    "    features = {\"base\": base, \"hypothesis\": hypothesis}\n",
    "    return features, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y))\n",
    "    dataset = dataset.map(rearrange)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = prepro_configs['vocab_size']\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "CONV_FEATURE_DIM = 300\n",
    "CONV_OUTPUT_DIM = 128\n",
    "CONV_WINDOW_SIZE = 3\n",
    "\n",
    "SIMILARITY_DENSE_FEATURE_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = VOCAB_SIZE\n",
    "embedding_size = WORD_EMBEDDING_DIM\n",
    "conv_channel_size = CONV_FEATURE_DIM\n",
    "conv_window_size = CONV_WINDOW_SIZE\n",
    "conv_output_feature_size = CONV_OUTPUT_DIM\n",
    "max_pool_window_size = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "similairiry_dense_dim = SIMILARITY_DENSE_FEATURE_DIM\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "\n",
    "    def basic_conv_sementic_network(inputs, name, reuse=False):\n",
    "        conv_layer = tf.keras.layers.Conv1D(conv_channel_size, \n",
    "                                            conv_window_size, \n",
    "                                            activation=tf.nn.relu, \n",
    "                                            name=name + 'conv_1d',\n",
    "                                            padding='same')(inputs)\n",
    "        \n",
    "        conv_layer = tf.keras.layers.Dropout(0.2)(conv_layer)\n",
    "        \n",
    "        max_pool_layer = tf.keras.layers.MaxPool1D(max_pool_window_size, 1)(conv_layer)\n",
    "        \n",
    "        output_layer = tf.layers.Dense(conv_output_feature_size, \n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name=name + 'dense')(max_pool_layer)\n",
    "        \n",
    "        output_layer = tf.keras.layers.Dropout(0.2)(output_layer)\n",
    "        \n",
    "        output_layer = tf.squeeze(output_layer, 1)\n",
    "        \n",
    "        return output_layer\n",
    "    \n",
    "    embedding = tf.keras.layers.Embedding(vocabulary_size,\n",
    "                                          embedding_size)\n",
    "    \n",
    "    base_embedded_matrix = embedding(features['base'])\n",
    "    hypothesis_embedded_matrix = embedding(features['hypothesis'])\n",
    "    \n",
    "    base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix)\n",
    "    hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix)\n",
    "    \n",
    "    base_sementic_matrix = basic_conv_sementic_network(base_embedded_matrix, 'base')\n",
    "    hypothesis_sementic_matrix = basic_conv_sementic_network(hypothesis_embedded_matrix, 'hypothesis')\n",
    "    \n",
    "    merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)\n",
    "\n",
    "    similarity_dense_layer = tf.keras.layers.Dense(similairiry_dense_dim,\n",
    "                                             activation=tf.nn.relu)(merged_matrix)\n",
    "    \n",
    "    similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer)\n",
    "    \n",
    "    logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer)\n",
    "    logit_layer = tf.squeeze(logit_layer, 1)\n",
    "    \n",
    "    if PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  predictions={\n",
    "                      'prob':tf.nn.sigmoid(logit_layer)\n",
    "                  })\n",
    "            \n",
    "    loss =tf.losses.sigmoid_cross_entropy(labels, logit_layer)\n",
    "    \n",
    "    elif EVAL:\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.nn.sigmoid(logit_layer))\n",
    "        eval_metric_ops = {'acc': accuracy}\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  eval_metric_ops= eval_metric_ops,\n",
    "                  loss=loss)\n",
    "    \n",
    "    elif TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "                  mode=mode,\n",
    "                  train_op=train_op,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "est = tf.estimator.Estimator(model_fn, \n",
    "                             model_dir=DATA_OUT_PATH + 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est.train(train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_Q1_DATA_FILE = 'test_q1.npy'\n",
    "TEST_Q2_DATA_FILE = 'test_q2.npy'\n",
    "\n",
    "test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'))\n",
    "test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"base\":test_q1_data, \n",
    "                                                         \"hypothesis\":test_q2_data}, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.array([p['prob'] for p in est.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DEFAULT_PATH='~/.kaggle/competitions/quora-question-pairs/'\n",
    "\n",
    "test = pd.read_csv(DEFAULT_PATH + \"test.csv\", encoding='utf-8')\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"test_id\":test[\"test_id\"], \"is_duplicate\": list(predictions)} )\n",
    "output.to_csv(DATA_OUT_PATH + \"cnn_predict.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
