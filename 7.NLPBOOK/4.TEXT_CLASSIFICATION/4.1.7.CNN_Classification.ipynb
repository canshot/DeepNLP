{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 카글 텍스트 분류 - 합성곱 신경망 활용 접근방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH ='~/.kaggle/competitions/word2vec-nlp-tutorial/'\n",
    "FILE_DIR_PATH = './data_in/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'\n",
    "INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'\n",
    "\n",
    "DATA_CONFIGS_FILE_NAME = 'data_configs.json'\n",
    "\n",
    "train_input_data = np.load(open(FILE_DIR_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "train_label_data = np.load(open(FILE_DIR_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))\n",
    "\n",
    "prepro_configs = None\n",
    "\n",
    "with open(FILE_DIR_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f:\n",
    "    prepro_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라메터 변수\n",
    "RNG_SEED = 1234\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "VOCAB_SIZE = len(prepro_configs)\n",
    "EMB_SIZE = 128\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_SEQ_LEN = 604 # 문장 최대 길이\n",
    "\n",
    "train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)\n",
    "\n",
    "#문장 길이 구하는 값, 전처리로 같은 길이를 맞추어 놔서 의미 없음\n",
    "train_len = np.array([min(len(x), MAX_SEQ_LEN) for x in input_train])\n",
    "eval_len = np.array([min(len(x), MAX_SEQ_LEN) for x in input_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(X, Y=None):\n",
    "    input, label = {'x': X}, Y\n",
    "    return input, label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_train))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_eval))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    #embedding layer를 선언합니다.\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "                    features['x'],\n",
    "                    VOCAB_SIZE,\n",
    "                    EMB_SIZE,\n",
    "                    initializer=params['embedding_initializer']\n",
    "                    )\n",
    "    \n",
    "    # 현재 모델이 학습모드인지 여부를 확인하는 변수입니다.\n",
    "    training = (mode == tf.estimator.ModeKeys.TRAIN)    \n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb = tf.layers.dropout(inputs=input_layer,\n",
    "                                   rate=0.5,\n",
    "                                   training=training)\n",
    "    \n",
    "    \n",
    "    conv = tf.layers.conv1d(\n",
    "            inputs=dropout_emb,\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu)\n",
    "    \n",
    "    pool = tf.reduce_max(input_tensor=conv, axis=1)\n",
    "    hidden = tf.layers.dense(inputs=pool, units=250, activation=tf.nn.relu)\n",
    "    dropout_hidden = tf.layers.dropout(inputs=hidden, rate=0.2, training=training)\n",
    "    logits = tf.layers.dense(inputs=dropout_hidden, units=1, name='logits')\n",
    "    \n",
    "    #prediction 진행 시, None\n",
    "    if labels is not None:\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    if tf.estimator.model-:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss = loss)\n",
    "    \n",
    "    elif EVAL:\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc': accuracy})\n",
    "        \n",
    "    elif PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'prob': tf.nn.sigmoid(logits)\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 100, '_session_config': None, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11d4e65c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-157\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 157 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.6775138, step = 158\n",
      "INFO:tensorflow:global_step/sec: 3.26437\n",
      "INFO:tensorflow:loss = 0.58203727, step = 258 (30.635 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 314 into /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7281116.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-10-27-05:15:33\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-314\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-10-27-05:15:37\n",
      "INFO:tensorflow:Saving dict for global step 314: acc = 0.7358, global_step = 314, loss = 0.5344597\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 314: /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.7358, 'loss': 0.5344597, 'global_step': 314}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0)}\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "config_tf = tf.estimator.RunConfig()\n",
    "config_tf._save_checkpoints_secs = 100\n",
    "config_tf._keep_checkpoint_max =  2\n",
    "config_tf._log_step_count_steps = 100\n",
    "\n",
    "cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir, config=config_tf, params=params)\n",
    "cnn_est.train(train_input_fn) #학습하기\n",
    "cnn_est.evaluate(eval_input_fn) #평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /Users/sinseongjin/github/DeepNLP/7.NLPBOOK/4.TEXT_CLASSIFICATION/data_out/checkpoint/cnn/model.ckpt-314\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# 예측된 모델을 불러 체크포인트로 결과치를 불러온다.\n",
    "test_input_data = np.load(open(FILE_DIR_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb')) #테스트데이터 로드\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False) #numpy 형태로 저장\n",
    "cnn_est.predict(input_fn=predict_input_fn)\n",
    "\n",
    "predictions = np.array([p['prob'][0] for p in cnn_est.predict(input_fn=predict_input_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#테스트 데이터 로드\n",
    "test = pd.read_csv(DEFAULT_PATH+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "print (\"test dataset shape: {}\".format(test.shape))\n",
    "\n",
    "#알아보기 쉽게 데이터랑 붙여두는 편이 좋을 거 같습니다.\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":list(predictions)} )\n",
    "\n",
    "#지금까지 처리한 결과를 파일로 저장합니다.\n",
    "output.to_csv(\"./data_out/Bag_of_Words_model_test.csv\", index=False, quoting=3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
